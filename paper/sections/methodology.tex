\section{Methodology}\label{sec:methodology}

\subsection{Narrative Prompting Framework}

Building on \cite{cunningham2024base}'s demonstration that narrative framing improves language model forecasting accuracy, I adopt a similar approach for policy prediction. My key insight is that language models perform better when asked to recount future events as if they were historical, rather than making direct predictions.

For example, rather than directly asking, "What will the PM2.5 level be under President X?", I construct a scenario where an EPA official in 2025 presents environmental outcomes. This approach offers several advantages:

\begin{itemize}
    \item It allows models to integrate domain knowledge naturally.
    \item It avoids conflicts with terms of service around prediction.
    \item It provides context for coherent scenario generation.
    \item It mirrors how real-world experts discuss outcomes.
\end{itemize}

\subsection{Models and Metrics}

I examine three large language models:
\begin{itemize}
    \item GPT-4o (OpenAI)
    \item GPT-4o-mini (OpenAI)
    \item Grok (xAI)
\end{itemize}

I also attempted to use Claude and Gemini models, but both declined to provide predictions on political outcomes.

For each model, I gather predictions on three key metrics for 2025. Table~\ref{tab:metrics} provides descriptions, data sources, and recent historical data ranges for each metric.

\begin{table}[htbp]
    \centering
    \small
    \caption{Metrics and Descriptions}
    \begin{tabular}{p{5cm} p{9cm}}
    \hline
    Metric & Description \\
    \hline
    PM2.5 ($\mu g/m^3$) & Annual mean of particulate matter concentration, indicating air quality and health impacts \\
    GDP per capita (2017 dollars) & Real GDP per capita, capturing economic performance in inflation-adjusted dollars \\
    Supplemental Poverty Measure (SPM) rate (\%) & Percentage of people in poverty, accounting for income, resources, and thresholds specific to the SPM \\
    \hline
    \end{tabular}
    \label{tab:metrics}
\end{table}

\subsection{Historical Context and Thresholds}

I prompted each model with historical data from 2010 to 2023 for each of the requested metrics, based on authoritative sources. For PM2.5, I used EPA data on air quality trends \cite{epa_pm25_trends}. For GDP per capita, I provided historical values from the U.S. Bureau of Economic Analysis \cite{fred_gdp_per_capita}. For the SPM rate, I used data and thresholds from the Census Bureauâ€™s supplemental poverty documentation \cite{census_spm_2023}, along with specific information on SPM resources and thresholds outlined in their technical documentation \cite{spm_techdoc}.

This context enabled each model to anchor its predictions in recent historical trends, enhancing the credibility and relevance of generated forecasts.

\subsection{Experimental Design}

For each combination of model, candidate, and metric, I conduct 500 trials using narrative prompts. The prompts vary by metric type:

\begin{itemize}
    \item \textbf{Economic metrics:} Senior Federal Reserve official giving a speech
    \item \textbf{Environmental metrics:} EPA Administrator presenting data
    \item \textbf{Social metrics:} Census Bureau economist reviewing outcomes
\end{itemize}

Each narrative is set in late 2025, allowing time for initial policy impacts while remaining within a reasonable forecasting horizon. I include consistent elements across trials to maintain structure:

\begin{itemize}
    \item Setting (e.g., conference presentation, agency briefing)
    \item Authority figure appropriate to the metric
    \item Request for specific numerical outcomes
    \item Context for broader policy discussion
\end{itemize}

\subsection{Statistical Framework}

I employ a regression framework with model interactions to examine:
\begin{itemize}
    \item Main effects of candidate choice on outcomes
    \item Differences between models in predicted effects
    \item Variance patterns across models and metrics
\end{itemize}

For each metric $m$, candidate $c$, and model $k$, I use the following specification:

\begin{equation}
    Y_{mcki} = \beta_0 + \beta_1 \text{Harris}_c + \gamma_k + \delta_k \text{Harris}_c + \epsilon_{mcki}
\end{equation}

where $Y_{mcki}$ is the predicted outcome for trial $i$, $\text{Harris}_c$ is an indicator for Kamala Harris as candidate, $\gamma_k$ are model fixed effects, and $\delta_k$ captures model-specific differences in the Harris effect.