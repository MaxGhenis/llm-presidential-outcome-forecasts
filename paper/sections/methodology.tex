\section{Methodology}\label{sec:methodology}

\subsection{Narrative Prompting Framework}

Building on \cite{cunningham2024base}'s demonstration that narrative framing improves language model forecasting accuracy, I adopt a similar approach for policy prediction. The key insight is that language models perform better when asked to tell stories about future events as if they were historical, rather than making direct predictions.

For example, rather than directly asking "What will the PM2.5 level be under President X?", I construct a scene where an EPA official in 2025 is giving a presentation reviewing environmental outcomes. This approach offers several advantages:

\begin{itemize}
    \item Allows models to integrate domain knowledge naturally
    \item Reduces apparent conflict with terms of service around prediction
    \item Provides context for coherent scenario generation
    \item Mimics how real-world experts discuss outcomes
\end{itemize}

\subsection{Models and Metrics}

I examine three large language models:
\begin{itemize}
    \item GPT-4 (OpenAI)
    \item GPT-4-mini (OpenAI)
    \item Grok (xAI)
\end{itemize}

For each model, I gather predictions on three key metrics for 2025:
\begin{itemize}
    \item PM2.5 air quality ($\mu g/m^3$): A key measure of air pollution with direct health impacts
    \item GDP per capita (2017 dollars): Capturing overall economic performance
    \item Supplemental Poverty Measure rate (\%): Reflecting distributional outcomes
\end{itemize}

These metrics were chosen to span different policy domains while having clear numerical outcomes and established measurement approaches.

\subsection{Experimental Design}

For each combination of model, candidate, and metric, I conduct 100 trials using narrative prompts. The prompts vary by metric type:

\begin{itemize}
    \item \textbf{Economic metrics:} Federal Reserve Chair Jerome Powell giving speeches
    \item \textbf{Environmental metrics:} EPA officials presenting data
    \item \textbf{Social metrics:} Policy analysts reviewing outcomes
\end{itemize}

Each narrative is set in late 2025, allowing time for initial policy impacts while remaining within a reasonable forecasting horizon. The prompts include consistent elements across trials:

\begin{itemize}
    \item Setting (e.g., conference presentation, agency briefing)
    \item Authority figure appropriate to the metric
    \item Request for specific numerical outcomes
    \item Context for broader policy discussion
\end{itemize}

\subsection{Statistical Framework}

The analysis employs a regression framework with model interactions to examine:
\begin{itemize}
    \item Main effects of candidate choice on outcomes
    \item Differences between models in predicted effects
    \item Variance patterns across models and metrics
\end{itemize}

For each metric $m$, candidate $c$, and model $k$, the basic specification is:

\begin{equation}
    Y_{mcki} = \beta_0 + \beta_1 \text{Harris}_c + \gamma_k + \delta_k \text{Harris}_c + \epsilon_{mcki}
\end{equation}

where $Y_{mcki}$ is the predicted outcome for trial $i$, $\text{Harris}_c$ is an indicator for Kamala Harris as candidate, $\gamma_k$ are model fixed effects, and $\delta_k$ captures model-specific differences in the Harris effect.