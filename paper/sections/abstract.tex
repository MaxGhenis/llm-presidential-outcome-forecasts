\begin{abstract}
    This study examines the use of large language models (LLMs) to forecast policy-relevant outcomes under different potential U.S. presidential administrations in 2025. Using a narrative prompting technique, I compare predictions from three AI models (GPT-4o, GPT-4o-mini, and Grok) across air quality, GDP per capita, and poverty rates. Results show significant predicted differences between administrations, with systematic variation in effect sizes across models. Grok consistently predicts larger differences than other models, while GPT-4o-mini shows more conservative estimates. The findings demonstrate both the potential and limitations of using language models for policy forecasting, while highlighting the importance of model selection and prompt design in extracting reliable predictions.
    \end{abstract}